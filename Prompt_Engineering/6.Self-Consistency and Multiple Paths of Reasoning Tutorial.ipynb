{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Consistency and Multiple Paths of Reasoning Tutorial\n",
    "\n",
    "Large language models can sometimes produce inconsistent or unreliable outputs. By leveraging multiple reasoning paths and aggregating results, we can enhance the robustness and accuracy of AI-generated responses. This approach is particularly useful for complex problem-solving tasks where a single path of reasoning might be insufficient or prone to errors.\n",
    "\n",
    "Key Components\n",
    "\n",
    "1. Generating multiple reasoning paths\n",
    "2. Aggregating results for better answers\n",
    "3. Implementing self-consistency checks\n",
    "4. Applying these techniques to various problem-solving scenarios\n",
    "5. Method Details\n",
    "\n",
    "Our approach involves the following steps:\n",
    "\n",
    "1. Designing prompts that encourage diverse reasoning paths\n",
    "2. Generating multiple responses using these prompts\n",
    "3. Implementing aggregation methods to combine and analyze the generated responses\n",
    "4. Applying self-consistency checks to evaluate the reliability of the results\n",
    "5. Demonstrating the effectiveness of this approach on various problem types\n",
    "6. Throughout the tutorial, we'll use practical examples to illustrate how these techniques can be applied to enhance the quality and reliability of AI-generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7f9dcfd167d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7f9dcec53e20>, model_name='gemma-7b-it', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "llm = ChatGroq(\n",
    "     groq_api_key = groq_api_key,\n",
    "     model = \"gemma-7b-it\"\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Multiple Reasoning Paths\n",
    "\n",
    "Let's create a function that generates multiple reasoning paths for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_paths(problem, num_paths=3):\n",
    "    \"\"\"\n",
    "    Generate multiple reasoning paths for a given problem.\n",
    "    \n",
    "    Args:\n",
    "    problem (str): The problem statement.\n",
    "    num_paths (int): Number of reasoning paths to generate.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of generated reasoning paths.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"problem\", \"path_number\"],\n",
    "        template=\"\"\"Solve the following problem using a unique approach. This is reasoning path {path_number}.\n",
    "        Problem: {problem}\n",
    "        Reasoning path {path_number}:\"\"\"\n",
    "    )\n",
    "\n",
    "    paths = []\n",
    "    for i in range(num_paths):\n",
    "        chain = prompt_template | llm\n",
    "        response = chain.invoke({\"problem\": problem, \"path_number\": i+1}).content\n",
    "        paths.append(response)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path 1:\n",
      "**Step 1: Identify the relevant factors**\n",
      "\n",
      "- Initial velocity (u) = 20 m/s\n",
      "- Gravity (g) = -9.8 m/sÂ² (downward)\n",
      "\n",
      "**Step 2: Recognize the conservation of energy**\n",
      "\n",
      "- Initial kinetic energy (kinetic energy of the ball at the highest point) = Potential energy at the highest point\n",
      "\n",
      "**Step 3: Apply the conservation of energy equation**\n",
      "\n",
      "$$\\frac{1}{2}mv^2 = mgh$$\n",
      "\n",
      "**Step 4: Solve for the height (h)**\n",
      "\n",
      "$$h = \\frac{v^2}{2g} = \\frac{(20 m/s)^2}{2(9.8 m/s^2)} = \\boxed{20.4 m}$$\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The ball will reach a height of 20.4 m.\n",
      "\n",
      "Path 2:\n",
      "**Step 1: Identify the relevant concepts.**\n",
      "\n",
      "- The height of the ball is influenced by its initial velocity and gravitational acceleration.\n",
      "- The height reached by a projectile is related to its potential energy.\n",
      "\n",
      "**Step 2: Apply the conservation of energy.**\n",
      "\n",
      "- The initial kinetic energy of the ball is converted into potential energy at the highest point of its trajectory.\n",
      "- The potential energy is given by: $$U = mgh$$ where m is the mass of the ball, g is the acceleration due to gravity, and h is the height.\n",
      "\n",
      "**Step 3: Substitute the known values.**\n",
      "\n",
      "- Initial velocity (v) = 20 m/s\n",
      "- Gravitational acceleration (g) = 9.8 m/s^2\n",
      "\n",
      "$$U = mgh = (0.5 kg)(9.8 m/s^2)h$$\n",
      "\n",
      "**Step 4: Solve for height (h).**\n",
      "\n",
      "$$h = \\frac{U}{mg} = \\frac{1/2mv^2}{mg} = \\frac{v^2}{2g}$$\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "$$h = \\frac{(20 m/s)^2}{2(9.8 m/s^2)} = \\boxed{20.4 m}$$\n",
      "\n",
      "Path 3:\n",
      "**Step 1: Identify the relevant principles**\n",
      "\n",
      "- Conservation of energy\n",
      "- Potential and kinetic energy relationships\n",
      "\n",
      "\n",
      "**Step 2: Visualize the process**\n",
      "\n",
      "- Imagine the ball as a point mass.\n",
      "- As the ball is thrown upwards, its kinetic energy is converted into potential energy.\n",
      "\n",
      "\n",
      "**Step 3: Apply the conservation of energy principle**\n",
      "\n",
      "- Initial kinetic energy (1/2mv^2) = Final potential energy (mgh)\n",
      "\n",
      "**Step 4: Substitute the values**\n",
      "\n",
      "- 1/2(0.1kg)(20m/s)^2 = (0.1kg)(9.8m/s^2)h\n",
      "\n",
      "**Step 5: Solve for h**\n",
      "\n",
      "- h = 20.4 m\n",
      "\n",
      "**Therefore, the ball will go approximately 20.4 meters high.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "problem = \"A ball is thrown upwards with an initial velocity of 20 m/s. How high will it go?\"\n",
    "paths = generate_multiple_paths(problem)\n",
    "\n",
    "for i, path in enumerate(paths, 1):\n",
    "    print(f\"Path {i}:\\n{path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating Results\n",
    "\n",
    "Now that we have multiple reasoning paths, let's create a function to aggregate the results and determine the most consistent answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(paths):\n",
    "    \"\"\"\n",
    "    Aggregate results from multiple reasoning paths.\n",
    "    \n",
    "    Args:\n",
    "    paths (list): List of reasoning paths.\n",
    "    \n",
    "    Returns:\n",
    "    str: The most consistent answer.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"paths\"],\n",
    "        template=\"\"\"Analyze the following reasoning paths and determine the most consistent answer. If there are discrepancies, explain why and provide the most likely correct answer.\n",
    "        Reasoning paths:\n",
    "        {paths}\n",
    "        \n",
    "        Most consistent answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"paths\": \"\\n\".join(paths)}).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Result:\n",
      " The most consistent answer is **20.4 m**.\n",
      "\n",
      "The reasoning paths provided all lead to the same conclusion: the ball will reach a height of 20.4 m. This is the most likely correct answer.\n"
     ]
    }
   ],
   "source": [
    "aggregated_result = aggregate_results(paths)\n",
    "print(\"Aggregated Result:\\n\", aggregated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Consistency Check\n",
    "\n",
    "To further improve our results, let's implement a self-consistency check that evaluates the reliability of our aggregated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_consistency_check(problem, aggregated_result):\n",
    "    \"\"\"\n",
    "    Perform a self-consistency check on the aggregated result.\n",
    "    \n",
    "    Args:\n",
    "    problem (str): The original problem statement.\n",
    "    aggregated_result (str): The aggregated result to check.\n",
    "    \n",
    "    Returns:\n",
    "    str: An evaluation of the result's consistency and reliability.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"problem\", \"result\"],\n",
    "        template=\"\"\"Evaluate the consistency and reliability of the following result for the given problem.\n",
    "        Problem: {problem}\n",
    "        Result: {result}\n",
    "        \n",
    "        Evaluation (consider factors like logical consistency, adherence to known facts, and potential biases):\"\"\"\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    response = chain.invoke({\"problem\": problem, \"result\": aggregated_result}).content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Consistency Evaluation:\n",
      " ## Consistency and Reliability Evaluation:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Logical consistency:** The result is consistent with the laws of physics, specifically the conservation of energy.\n",
      "* **Adherence to known facts:** The result is in agreement with the known formula for the height reached by a projectile launched with an initial velocity.\n",
      "* **Specificity:** The result provides a precise numerical value (20.4 m) instead of a vague range.\n",
      "\n",
      "**Weaknesses:**\n",
      "\n",
      "* **Limited information:** The evaluation lacks further details about the reasoning process or the specific assumptions made.\n",
      "* **Potential bias:** The statement suggests the most likely correct answer, implying a degree of certainty that might not be fully justified.\n",
      "\n",
      "**Overall:**\n",
      "\n",
      "The result is **consistent** and **reliable** based on its adherence to known principles and facts. However, the lack of additional information and the suggestion of certainty require further scrutiny and confirmation with more detailed analysis or experimental data.\n",
      "\n",
      "**Recommendations for improvement:**\n",
      "\n",
      "* Provide a more detailed breakdown of the reasoning steps and assumptions made.\n",
      "* Acknowledge any limitations or uncertainties in the analysis.\n",
      "* Include relevant data or references to support the claimed result.\n"
     ]
    }
   ],
   "source": [
    "consistency_evaluation = self_consistency_check(problem, aggregated_result)\n",
    "print(\"Self-Consistency Evaluation:\\n\", consistency_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_inv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
