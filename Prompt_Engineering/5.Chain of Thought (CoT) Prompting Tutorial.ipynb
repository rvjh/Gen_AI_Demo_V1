{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain of Thought (CoT) Prompting Tutorial\n",
    "\n",
    "As AI language models become more advanced, there's an increasing need to guide them towards producing more transparent, logical, and verifiable outputs. CoT prompting addresses this need by encouraging models to show their work, much like how humans approach complex problem-solving tasks. This technique not only improves the accuracy of AI responses but also makes them more interpretable and trustworthy.\n",
    "\n",
    "Key Components\n",
    "\n",
    "1. Basic CoT Prompting: Introduction to the concept and simple implementation.\n",
    "2. Advanced CoT Techniques: Exploring more sophisticated CoT approaches.\n",
    "3. Comparative Analysis: Examining the differences between standard and CoT prompting.\n",
    "4. Problem-Solving Applications: Applying CoT to various complex tasks.\n",
    "\n",
    "Method Details\n",
    "\n",
    "The tutorial will guide learners through the following methods:\n",
    "\n",
    "1. Basic CoT Implementation: We'll create simple CoT prompts and compare their outputs to standard prompts.\n",
    "\n",
    "2. Advanced CoT Techniques: We'll explore more complex CoT strategies, including multi-step reasoning and self-consistency checks.\n",
    "\n",
    "3. Practical Applications: We'll apply CoT prompting to various problem-solving scenarios, such as mathematical word problems and logical reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7fb35b0f03d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7fb35b0f3a00>, model_name='gemma-7b-it', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "llm = ChatGroq(\n",
    "     groq_api_key = groq_api_key,\n",
    "     model = \"gemma-7b-it\"\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Chain of Thought Prompting\n",
    "\n",
    "Let's start with a simple example to demonstrate the difference between a standard prompt and a Chain of Thought prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Response:\n",
      "Average speed = Distance / Time\n",
      "\n",
      "= 120 km / 2 h\n",
      "= **60 km/h**\n",
      "\n",
      "Therefore, the average speed of the train is 60 km/h.\n",
      "\n",
      "Chain of Thought Response:\n",
      "**Step 1: Recall the formula for average speed.**\n",
      "\n",
      "Average Speed = Distance / Time\n",
      "\n",
      "**Step 2: Substitute the given values into the formula.**\n",
      "\n",
      "Distance = 120 km\n",
      "Time = 2 hours\n",
      "\n",
      "Average Speed = 120 km / 2 hours\n",
      "**Average Speed = 60 km/h**\n",
      "\n",
      "Therefore, the average speed of the train is **60 km/h**.\n"
     ]
    }
   ],
   "source": [
    "# Standard prompt\n",
    "standard_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question conciesly: {question}.\"\n",
    ")\n",
    "\n",
    "# Chain of Thought prompt\n",
    "cot_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question step by step conciesly: {question}\"\n",
    ")\n",
    "\n",
    "# Create chains\n",
    "standard_chain = standard_prompt | llm\n",
    "cot_chain = cot_prompt | llm\n",
    "\n",
    "# Example question\n",
    "question = \"If a train travels 120 km in 2 hours, what is its average speed in km/h?\"\n",
    "\n",
    "# Get responses\n",
    "standard_response = standard_chain.invoke(question).content\n",
    "cot_response = cot_chain.invoke(question).content\n",
    "\n",
    "print(\"Standard Response:\")\n",
    "print(standard_response)\n",
    "print(\"\\nChain of Thought Response:\")\n",
    "print(cot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Chain of Thought Techniques\n",
    "\n",
    "Now, let's explore a more advanced CoT technique that encourages multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: State what you're going to calculate**\n",
      "\n",
      "We need to calculate the average speed of the entire journey.\n",
      "\n",
      "**Step 2: Write the formula**\n",
      "\n",
      "Average speed = Total distance / Total time taken\n",
      "\n",
      "**Step 3: Perform the calculation**\n",
      "\n",
      "Distance travelled at 60 km/h = 150 km\n",
      "Distance travelled at 50 km/h = 100 km\n",
      "\n",
      "Total distance = 150 km + 100 km = 250 km\n",
      "\n",
      "Time taken at 60 km/h = 150 km / 60 km/h = 2.5 hours\n",
      "Time taken at 50 km/h = 100 km / 50 km/h = 2 hours\n",
      "\n",
      "Total time taken = 2.5 hours + 2 hours = 4.5 hours\n",
      "\n",
      "Average speed = 250 km / 4.5 hours = **55.56 km/h**\n",
      "\n",
      "**Step 4: Explain the result**\n",
      "\n",
      "The average speed for the entire journey is 55.56 km/h.\n"
     ]
    }
   ],
   "source": [
    "advanced_cot_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Solve the following problem step by step. For each step:\n",
    "1. State what you're going to calculate\n",
    "2. Write the formula you'll use (if applicable)\n",
    "3. Perform the calculation\n",
    "4. Explain the result\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Solution:\"\"\"\n",
    ")\n",
    "\n",
    "advanced_cot_chain = advanced_cot_prompt | llm\n",
    "\n",
    "complex_question = \"A car travels 150 km at 60 km/h, then another 100 km at 50 km/h. What is the average speed for the entire journey?\"\n",
    "\n",
    "advanced_cot_response = advanced_cot_chain.invoke(complex_question).content\n",
    "print(advanced_cot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparative Analysis\n",
    "\n",
    "Let's compare the effectiveness of standard prompting vs. CoT prompting on a more challenging problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Response:\n",
      "**Step 1: Calculate the volume of the tank.**\n",
      "\n",
      "Volume = πr²h\n",
      "= (3.14159)(1.5)²(4)\n",
      "= 14.13 cubic meters\n",
      "\n",
      "**Step 2: Calculate the volume of water in the tank.**\n",
      "\n",
      "2/3 * 14.13 = 9.42 cubic meters\n",
      "\n",
      "**Step 3: Calculate the rate of water being added.**\n",
      "\n",
      "10 liters/min = 0.01 cubic meters/min\n",
      "\n",
      "**Step 4: Calculate the time it will take for the tank to overflow.**\n",
      "\n",
      "9.42 / 0.01 = 942 minutes\n",
      "= **15 hours 22 minutes**\n",
      "\n",
      "Chain of Thought Response:\n",
      "**Step 1: What are we going to calculate?**\n",
      "\n",
      "We need to find the time it takes for the water tank to overflow.\n",
      "\n",
      "**Step 2: Formula**\n",
      "\n",
      "$$V = \\pi r^2h$$\n",
      "\n",
      "Where:\n",
      "- V is the volume of the cylinder (in cubic meters)\n",
      "- r is the radius of the cylinder (in meters)\n",
      "- h is the height of the cylinder (in meters)\n",
      "\n",
      "**Step 3: Perform the calculation**\n",
      "\n",
      "$$V = (3.14159)(1.5)^2(4) = 14.13 m^3$$\n",
      "\n",
      "**Step 4: Explanation**\n",
      "\n",
      "The tank can hold 14.13 cubic meters of water.\n",
      "\n",
      "**Step 5: Given rate of water addition**\n",
      "\n",
      "The water is being added at a rate of 10 liters per minute, which is equal to 0.01 cubic meters per minute.\n",
      "\n",
      "**Step 6: Time to overflow**\n",
      "\n",
      "$$Time = \\frac{Volume}{Rate}$$\n",
      "\n",
      "$$Time = \\frac{14.13 m^3}{0.01 m^3/min} = 1413 minutes$$\n",
      "\n",
      "**Step 7: Convert minutes to hours and minutes**\n",
      "\n",
      "$$1413 minutes = (1413/60) hours = 23.55 hours$$\n",
      "\n",
      "**Therefore, it will take approximately 23 hours and 30 minutes for the tank to overflow.**\n"
     ]
    }
   ],
   "source": [
    "challenging_question = \"\"\"\n",
    "A cylindrical water tank with a radius of 1.5 meters and a height of 4 meters is 2/3 full. \n",
    "If water is being added at a rate of 10 liters per minute, how long will it take for the tank to overflow? \n",
    "Give your answer in hours and minutes, rounded to the nearest minute. \n",
    "(Use 3.14159 for π and 1000 liters = 1 cubic meter)\"\"\"\n",
    "\n",
    "standard_response = standard_chain.invoke(challenging_question).content\n",
    "cot_response = advanced_cot_chain.invoke(challenging_question).content\n",
    "\n",
    "print(\"Standard Response:\")\n",
    "print(standard_response)\n",
    "print(\"\\nChain of Thought Response:\")\n",
    "print(cot_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem-Solving Applications\n",
    "\n",
    "Now, let's apply CoT prompting to a more complex logical reasoning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Analysis:\n",
      "\n",
      "**Facts:**\n",
      "\n",
      "- Three people: Amy, Bob, Charlie.\n",
      "- One always tells the truth, one always lies, one alternates between truth and lies.\n",
      "- Statements:\n",
      "    - Amy: \"Bob is a liar.\"\n",
      "    - Bob: \"Charlie alternates between truth and lies.\"\n",
      "    - Charlie: \"Amy and I are both liars.\"\n",
      "\n",
      "\n",
      "**Possible Roles/Conditions:**\n",
      "\n",
      "- **Truth-teller:** Always tells the truth.\n",
      "- **Liar:** Always lies.\n",
      "- **Alternator:** Switches between truth and lies.\n",
      "\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "- Each statement is made honestly (consistent with their assigned role).\n",
      "\n",
      "\n",
      "**Possible Scenarios:**\n",
      "\n",
      "1. Amy is the truth-teller, Bob is the liar, Charlie is the alternator.\n",
      "2. Amy is the liar, Bob is the truth-teller, Charlie is the alternator.\n",
      "3. Amy is the alternator, Bob is the liar, Charlie is the truth-teller.\n",
      "\n",
      "\n",
      "**Testing Scenarios:**\n",
      "\n",
      "- **Scenario 1:** Leads to a contradiction: Charlie cannot be both the alternator and the truth-teller.\n",
      "- **Scenario 2:** Consistent.\n",
      "- **Scenario 3:** Leads to a contradiction: Charlie cannot be both the alternator and the liar.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "**Roles/Conditions:**\n",
      "\n",
      "- Amy: **Alternator**\n",
      "- Bob: **Truth-teller**\n",
      "- Charlie: **Liar**\n",
      "\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The consistency of Scenario 2 is confirmed by the truthfulness of Bob's statement and Charlie's lie, aligning with their assigned roles. This scenario also satisfies the constraint of the puzzle. Therefore, this is the only possible solution.\n"
     ]
    }
   ],
   "source": [
    "logical_reasoning_prompt = PromptTemplate(\n",
    "    input_variables=[\"scenario\"],\n",
    "    template=\"\"\"Analyze the following logical puzzle thoroughly. Follow these steps in your analysis:\n",
    "\n",
    "List the Facts:\n",
    "\n",
    "Summarize all the given information and statements clearly.\n",
    "Identify all the characters or elements involved.\n",
    "Identify Possible Roles or Conditions:\n",
    "\n",
    "Determine all possible roles, behaviors, or states applicable to the characters or elements (e.g., truth-teller, liar, alternator).\n",
    "Note the Constraints:\n",
    "\n",
    "Outline any rules, constraints, or relationships specified in the puzzle.\n",
    "Generate Possible Scenarios:\n",
    "\n",
    "Systematically consider all possible combinations of roles or conditions for the characters or elements.\n",
    "Ensure that all permutations are accounted for.\n",
    "Test Each Scenario:\n",
    "\n",
    "For each possible scenario:\n",
    "Assume the roles or conditions you've assigned.\n",
    "Analyze each statement based on these assumptions.\n",
    "Check for consistency or contradictions within the scenario.\n",
    "Eliminate Inconsistent Scenarios:\n",
    "\n",
    "Discard any scenarios that lead to contradictions or violate the constraints.\n",
    "Keep track of the reasoning for eliminating each scenario.\n",
    "Conclude the Solution:\n",
    "\n",
    "Identify the scenario(s) that remain consistent after testing.\n",
    "Summarize the findings.\n",
    "Provide a Clear Answer:\n",
    "\n",
    "State definitively the role or condition of each character or element.\n",
    "Explain why this is the only possible solution based on your analysis.\n",
    "Scenario:\n",
    "\n",
    "{scenario}\n",
    "\n",
    "Analysis:\"\"\")\n",
    "\n",
    "logical_reasoning_chain = logical_reasoning_prompt | llm\n",
    "\n",
    "logical_puzzle = \"\"\"In a room, there are three people: Amy, Bob, and Charlie. \n",
    "One of them always tells the truth, one always lies, and one alternates between truth and lies. \n",
    "Amy says, 'Bob is a liar.' \n",
    "Bob says, 'Charlie alternates between truth and lies.' \n",
    "Charlie says, 'Amy and I are both liars.' \n",
    "Determine the nature (truth-teller, liar, or alternator) of each person.\"\"\"\n",
    "\n",
    "logical_reasoning_response = logical_reasoning_chain.invoke(logical_puzzle).content\n",
    "print(logical_reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_inv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
